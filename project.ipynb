{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917e9bb1-b1dd-4ebf-97a5-93c56387bda5",
   "metadata": {},
   "source": [
    "# Classifying Pulsar Candidates Using a K-NN Algorithm\n",
    "\n",
    "## Introduction\n",
    "- Pulsars are rotating neutron stars observed to have pulses of radiation at very regular intervals, typically ranging from milliseconds to seconds. These accelerated particles produce very powerful beams of light, with some pulsars producing enough radio emissions to be detected on Earth (Ng et al., 2015).\n",
    "- Detection of Pulsars is a complex task and involves discerning fleeting Pulsar signals from a copious amount of background radio frequency interference (Eatough et al., 2010). To aid in the detection of Pulsars, computer algorithms can speed up the process and accuracy. Within our project, we aim to create a supervised *K*-Nearest Neighbors algorithm in the binary classification of the Pulsar star state.\n",
    "\n",
    "\n",
    "\n",
    "**Predictive question:** Can we create a *K*-nearest neighbor algorithm capable of classifying whether an observation is a Pulsar star or not, given the kurtosis and skewness of the integrated profile, as well as the mean, kurtosis, and skewness of the DM-SNR curve?\n",
    "\n",
    "\n",
    "#### Dataset\n",
    "- The data set that we use describes a sample of pulsar candidates collected as part of the High Time Resolution Universe Survey of the southern hemisphere.\n",
    "- The data set contains a total of 8 different features and 1 class variable. The variables are measures of the dispersion measure-signal-to-noise ratio (DM-SNR) or the integrated profile of each recorded observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e247f2-3d98-4ab9-a3f1-81736d03af1f",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84627c0e-2752-40d7-8feb-34fb72e799b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“package ‘ggplot2’ was built under R version 4.3.2”\n",
      "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.3     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2  \u001b[39m 3.5.0     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.2     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.2     \n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n",
      "── \u001b[1mAttaching packages\u001b[22m ────────────────────────────────────── tidymodels 1.1.1 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mbroom       \u001b[39m 1.0.5     \u001b[32m✔\u001b[39m \u001b[34mrsample     \u001b[39m 1.2.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdials       \u001b[39m 1.2.0     \u001b[32m✔\u001b[39m \u001b[34mtune        \u001b[39m 1.1.2\n",
      "\u001b[32m✔\u001b[39m \u001b[34minfer       \u001b[39m 1.0.5     \u001b[32m✔\u001b[39m \u001b[34mworkflows   \u001b[39m 1.1.3\n",
      "\u001b[32m✔\u001b[39m \u001b[34mmodeldata   \u001b[39m 1.2.0     \u001b[32m✔\u001b[39m \u001b[34mworkflowsets\u001b[39m 1.0.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mparsnip     \u001b[39m 1.1.1     \u001b[32m✔\u001b[39m \u001b[34myardstick   \u001b[39m 1.2.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mrecipes     \u001b[39m 1.0.8     \n",
      "\n",
      "── \u001b[1mConflicts\u001b[22m ───────────────────────────────────────── tidymodels_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mscales\u001b[39m::\u001b[32mdiscard()\u001b[39m masks \u001b[34mpurrr\u001b[39m::discard()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m   masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mfixed()\u001b[39m  masks \u001b[34mstringr\u001b[39m::fixed()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m      masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[31m✖\u001b[39m \u001b[34myardstick\u001b[39m::\u001b[32mspec()\u001b[39m masks \u001b[34mreadr\u001b[39m::spec()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mstep()\u001b[39m   masks \u001b[34mstats\u001b[39m::step()\n",
      "\u001b[34m•\u001b[39m Use \u001b[32mtidymodels_prefer()\u001b[39m to resolve common conflicts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries and setting seed\n",
    "library(tidyverse)\n",
    "library(tidymodels)\n",
    "install.packages(\"themis\") \n",
    "\n",
    "\n",
    "set.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b064a9-275a-4ef0-b013-93c650cc47d6",
   "metadata": {},
   "source": [
    "#### Reading and wrangling the data from internet:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53190306-995c-4691-8ebd-cb97fdb41120",
   "metadata": {},
   "source": [
    "First, we create two helper functions to keep track of the table numbers and graph number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b5a41-f426-47f7-8769-fbd3d8c4db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_count <- 0\n",
    "print_table_legend <- function(text) {\n",
    "    table_count <<- table_count + 1\n",
    "    print(paste(\"Table\", table_count,\":\",text))\n",
    "}\n",
    "\n",
    "graph_count <- 0\n",
    "print_graph_count <- function() {\n",
    "    graph_count <<- graph_count + 1\n",
    "    print(paste(\"Graph -\", graph_count))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e9470-1645-4809-93f9-75011b3f3a88",
   "metadata": {},
   "source": [
    "Since the initial data file is in a zip folder, we created a new repository on github, and utilized the raw link to demonstrate that the data can be read from the web. Then, we wrangle and clean the data. First, we rename the columns to give them descriptive names.  Since we are interested in predicting the class of the star, we convert the class column to the type factor. Then, we select only the required columns (Profile_skewness, Profile_kurtosis, DM_mean, DM_skewness, DM_kurtosis, Class) and we remove the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee048b5d-9fb0-4dce-a0b9-2eede25ad6bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in select(mutate(mutate(rename(read_csv(\"https://raw.githubusercontent.com/MichaelZhang33/HTRU21.csv/main/HTRU_2.arff\", : could not find function \"select\"\n",
     "output_type": "error",
     "traceback": [
      "Error in select(mutate(mutate(rename(read_csv(\"https://raw.githubusercontent.com/MichaelZhang33/HTRU21.csv/main/HTRU_2.arff\", : could not find function \"select\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "pulsar_df <- read_csv(\"https://raw.githubusercontent.com/MichaelZhang33/HTRU21.csv/main/HTRU_2.arff\", \n",
    "                       skip = 11, col_names = FALSE) |>\n",
    "     rename(\"Profile_mean\" = X1,\n",
    "        \"Profile_stdev\" = X2,\n",
    "        \"Profile_skewness\" = X3,\n",
    "        \"Profile_kurtosis\" = X4,\n",
    "        \"DM_mean\" = X5, \n",
    "        \"DM_stdev\" = X6, \n",
    "        \"DM_skewness\" = X7,\n",
    "        \"DM_kurtosis\" = X8,\n",
    "        \"class\" = X9) |>\n",
    "    mutate(class = as_factor(class)) |>\n",
    "    mutate(Class = fct_recode(class, \"notpulsar\" = \"0\", \"pulsar\" = \"1\")) |>\n",
    "    select(-Profile_mean, -Profile_stdev, -DM_stdev, -class)\n",
    "\n",
    "head(pulsar_df)\n",
    "print_table_legend(\"Wrangled and cleaned data for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b984db1-38f1-4290-93b1-aaa735788448",
   "metadata": {},
   "source": [
    "Now, we split the data into training and testing data. We train our model using 75% of the data, and test with the other 25% of the data. We stratify our data based on `Class`, because that is what we are interested in predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "276a38b2-6285-4af4-98ed-289ed9efd42c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in initial_split(pulsar_df, prop = 0.75, strata = Class): could not find function \"initial_split\"\n",
     "output_type": "error",
     "traceback": [
      "Error in initial_split(pulsar_df, prop = 0.75, strata = Class): could not find function \"initial_split\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into a training and testing set\n",
    "pulsar_split <- initial_split(pulsar_df, prop = 0.75, strata = Class)\n",
    "pulsar_train <- training(pulsar_split)\n",
    "pulsar_test <- testing(pulsar_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807f1503-23ef-40ef-97d7-435e260a7183",
   "metadata": {},
   "source": [
    "### Useful Tabular Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262e73da-a647-4f77-ab56-6497e95bb256",
   "metadata": {},
   "source": [
    "##### 1. Summary table for the number of pulsar star observations and non-pulsar star observations in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d8cbe-f618-431c-b35f-7e7169907af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_df_1 <- pulsar_train |>\n",
    "    group_by(Class) |>\n",
    "    summarize(count = n())\n",
    "useful_df_1\n",
    "\n",
    "\n",
    "print_table_legend(\"Number of pulsars and non-pulsars in the data set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cd829c-48b9-40a1-a2e2-5f223c266044",
   "metadata": {},
   "source": [
    "These results indicate that the dataset is not balanced, and classification algorithms can perform badly when trained on imbalanced datasets. Oversampling can be a useful way of overcoming the class imbalance and hence improving the model’s performance. Hence, will require oversampling of the positive pulsar cases when creating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ec4de7-feb4-4b39-961f-defba8aa1628",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(themis)\n",
    "balance_recipe <- recipe(Class ~ ., data = pulsar_train) |>\n",
    "  step_upsample(Class, over_ratio = 1, skip = FALSE) |>\n",
    "    prep()\n",
    "\n",
    "pulsar_train <- bake(balance_recipe, pulsar_train)\n",
    "pulsar_train |>\n",
    "    group_by(Class) |>\n",
    "    summarize(n = n())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0912c552-81bb-42f9-806e-3c63749e2bf0",
   "metadata": {},
   "source": [
    "##### 2. Summary table for the average value of each predictor variable based on class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6947b56-7465-4da5-a5e1-75c52df6dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_df_2 <- pulsar_train |>\n",
    "    group_by(Class) |>\n",
    "    summarize(mean_profile_skewness = mean(Profile_skewness), \n",
    "              mean_profile_kurtosis = mean(Profile_kurtosis), \n",
    "              mean_DM_mean = mean(DM_mean), \n",
    "              mean_DM_skewness = mean(DM_skewness), \n",
    "              mean_DM_kurtosis = mean(DM_kurtosis))\n",
    "useful_df_2\n",
    "print_table_legend(\"Average value of each predictor in the data set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55d02e2-fb4e-4807-a2bb-7f73c424cd3c",
   "metadata": {},
   "source": [
    "##### 3. Number of rows in each column that have na values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7fbc01-78cd-4882-855a-4bec825e08d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_df_3 <- colSums(is.na(pulsar_train))\n",
    "useful_df_3\n",
    "print_table_legend(\"Number of n/a values in the table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14663b6f-60f7-4591-a1fa-1dde1ff6fca5",
   "metadata": {},
   "source": [
    "### Useful visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b711cfc5-d0f8-49bd-957a-3704f4388691",
   "metadata": {},
   "source": [
    "#### 1. Distribution of Profile_skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b10cb-0d80-4d6d-b7a2-8158504523fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_sk_distribution <- pulsar_train |>\n",
    "    select(Profile_skewness, Class) |>\n",
    "    ggplot(aes(x = Profile_skewness)) +\n",
    "    geom_histogram(bins = 10) +\n",
    "    labs(x= \"Skewness of the integrated profile\") +\n",
    "    ggtitle(\"Distribution of the skewness of the integrated profile\") +\n",
    "    theme(text = element_text(size = 12)) +\n",
    "    facet_grid(cols = vars(Class))\n",
    "profile_sk_distribution\n",
    "\n",
    "print_graph_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb4f325-31d1-414a-8fcb-96b5b0e62a85",
   "metadata": {},
   "source": [
    "#### 2. Distribution of Profile_kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5605b66-7c7c-4693-a035-0c22a8fc218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_kur_distribution <- pulsar_train |>\n",
    "    select(Profile_kurtosis, Class) |>\n",
    "    ggplot(aes(x = Profile_kurtosis)) +\n",
    "    geom_histogram(bins = 10) +\n",
    "    labs(x= \"Excess kurtosis of integrated profile\") +\n",
    "    ggtitle(\"Distribution of the excess kurtosis of integrated profile\") +\n",
    "    theme(text = element_text(size = 15)) +\n",
    "    facet_grid(cols = vars(Class))\n",
    "profile_kur_distribution\n",
    "print_graph_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d9b6bb-327c-4e43-9905-b1cca5832273",
   "metadata": {},
   "source": [
    "#### 3. Distribution of DM_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35f2e29-4146-499f-902f-212c623de46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_dmMean_distribution <- pulsar_train |>\n",
    "    select(DM_mean, Class) |>\n",
    "    ggplot(aes(x = DM_mean)) +\n",
    "    geom_histogram(bins = 10) +\n",
    "    labs(x= \"Mean of DM-SNR curve\") +\n",
    "    ggtitle(\"Distribution of the mean of DM-SNR curve\") +\n",
    "    theme(text = element_text(size = 15)) +\n",
    "    facet_grid(cols = vars(Class))\n",
    "profile_dmMean_distribution\n",
    "print_graph_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a7d9ae-ccb8-4bdc-acbb-9263437c9685",
   "metadata": {},
   "source": [
    "#### 4. Distribution of DM_skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dd4508-e716-4789-b5e8-387288c6524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_dmsk_distribution <- pulsar_train |>\n",
    "    select(DM_skewness, Class) |>\n",
    "    ggplot(aes(x = DM_skewness)) +\n",
    "    geom_histogram(bins = 10) +\n",
    "    labs(x= \"Skewness of DM-SNR curve\") +\n",
    "    ggtitle(\"Distribution of the skewness of DM-SNR curve\") +\n",
    "    theme(text = element_text(size = 15)) +\n",
    "    facet_grid(cols = vars(Class))\n",
    "profile_dmsk_distribution\n",
    "print_graph_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976c31e3-ed42-4f2e-ae2a-e0db4df5713b",
   "metadata": {},
   "source": [
    "#### 5. Distribution of DM_kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f650b8a-676b-4015-9b05-9cc6931711c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_dmKur_distribution <- pulsar_train |>\n",
    "    select(DM_kurtosis, Class) |>\n",
    "    ggplot(aes(x = DM_kurtosis)) +\n",
    "    geom_histogram(bins = 10) +\n",
    "    labs(x= \"Kurtosis of DM-SNR curve\") +\n",
    "    ggtitle(\"Distribution of the kurtosis of DM-SNR curve\") +\n",
    "    theme(text = element_text(size = 15)) +\n",
    "    facet_grid(cols = vars(Class))\n",
    "profile_dmKur_distribution\n",
    "print_graph_count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35169e2e-e182-4d39-9d72-cef7cb34fc71",
   "metadata": {},
   "source": [
    "## Methods and Results\n",
    "\n",
    "### Methods\n",
    "To answer our question, we will utilize some of the columns from the initial dataset. These include:\n",
    " \n",
    "1. The excess kurtosis of the integrated profile (*Profile_kurtosis*)\n",
    "2. The skewness of the integrated profile (*Profile_skewness*)\n",
    "3. Mean of the DM-SNR curve (*DM_mean*)\n",
    "4. Excess kurtosis of the DM-SNR curve (*DM_kurtosis*)\n",
    "5. Skewness of the DM-SNR curve (*DM_skewness*)\n",
    "6. Observation Class (the outcome variable)\n",
    "\n",
    "Using the first 5 columns, we will predict through *classification* to determine the observation class.\n",
    "These variables were chosen based on the nature of Pulsar stars as powerful yet fleeting signals, where the kurtosis and skewness of the integrated profile will help distinguish the transient nature of the star. Furthermore, the DM-SNR mean, kurtosis and skewness will indicate the strength and nature of the signal in comparison to background noise and radio frequency interference.\n",
    "\n",
    "\n",
    "**Visualisation:** We will visualize our results by creating a confusion matrix to visualize the performance of the model. Furthermore, we will utilize histogram to visualize the relationship between the predictor variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c3d521-dd8b-4f5c-8380-500df1c5ec1b",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a92b31-722c-401b-9e1f-2c6101956e2d",
   "metadata": {},
   "source": [
    "First we create a workflow to make our model. We will use 20 values of K, lying between K=1 to K=100, and find the best value of K for our model. Then, we perform 5-fold validation, which improves the choice of K as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d6d5e-2d3a-46c6-81d5-11084bcb9917",
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.height = 5, repr.plot.width = 6)\n",
    "\n",
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) |>\n",
    "    set_engine(\"kknn\") |>\n",
    "    set_mode(\"classification\")\n",
    "\n",
    "knn_recipe <- recipe(Class ~ ., data = pulsar_train)\n",
    "knn_vfold <- vfold_cv(pulsar_train, v = 5, strata = Class)\n",
    "\n",
    "knn_wkflw <- workflow() |>\n",
    "  add_recipe(knn_recipe) |>\n",
    "  add_model(knn_spec)\n",
    "\n",
    "gridvals <- tibble(neighbors = seq(from = 1, to = 100, by = 5))\n",
    "\n",
    "knn_results <- workflow() |>\n",
    "  add_recipe(knn_recipe) |>\n",
    "  add_model(knn_spec) |>\n",
    "  tune_grid(resamples = knn_vfold, grid = gridvals) |>\n",
    "  collect_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daec1efd-c0b5-44e3-b07e-e8dd3e9681bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies <- knn_results |>\n",
    "  filter(.metric == \"accuracy\")\n",
    "accuracies\n",
    "print_table_legend(\"Accuracy of classifier for each K-value in range 1 to 100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c4dee-7d41-4ee5-894b-0a0706582c5a",
   "metadata": {},
   "source": [
    "Now, we create a graph to plot the accuracies of our model for each value of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9fd79a-6780-4778-add4-19eaa747fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_plot <- ggplot(accuracies, aes(x = neighbors, y = mean)) +\n",
    "  geom_point() +\n",
    "  geom_line() +\n",
    "  labs(x = \"Neighbors\", y = \"Accuracy Estimate\") +\n",
    "  ylim(0.90, 1) +\n",
    "  theme(text = element_text(size = 12))\n",
    "\n",
    "cross_val_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4b0eb7-a5b7-4a69-9c91-d69408cb12fe",
   "metadata": {},
   "source": [
    "Clearly, the accuracy is highest for $K=1$, but choosing $K=1$ will lead to overfitting. Also, the value of accuracy changes a lot for $K<25$. Because of these reasons, we will choose $K=30$, because the accuracy is fairly high ($95%$), and changing the value of $K$ does not change the the accuracy by a large amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca63d79-808d-4795-ab43-6ab8c2b57b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 30) |>\n",
    "    set_engine(\"kknn\") |>\n",
    "    set_mode(\"classification\")\n",
    "\n",
    "final_fit <- workflow() |>\n",
    "  add_recipe(knn_recipe) |>\n",
    "  add_model(final_spec) |>\n",
    "  fit(data = pulsar_train)\n",
    "\n",
    "pulsar_predictions <- predict(final_fit, pulsar_test) |>\n",
    "    bind_cols(pulsar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb6b2b-30e0-470f-9274-071776ce8fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_conf_mat <- pulsar_predictions |>\n",
    "     conf_mat(truth = Class, estimate = .pred_class)\n",
    "mnist_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "592ab9fc-dbbe-46c1-b78a-889280804061",
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in pull(filter(metrics(pulsar_predictions, truth = Class, estimate = .pred_class), : could not find function \"pull\"\n",
     "output_type": "error",
     "traceback": [
      "Error in pull(filter(metrics(pulsar_predictions, truth = Class, estimate = .pred_class), : could not find function \"pull\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "mnist_accuracy <- pulsar_predictions |>\n",
    "    metrics(truth = Class, estimate = .pred_class) |>\n",
    "    filter(.metric == \"accuracy\") |>\n",
    "    pull(.estimate)\n",
    "mnist_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd998323-07dc-4465-8141-c9d3e6eb4562",
   "metadata": {},
   "source": [
    "Since we are interested in predicting pulsars, we will assume that \"pulsar\" classification is the \"positive\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4683155-ee67-42dd-9387-bf79f475d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pulsar_predictions |> pull(Class) |> levels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c68747a-9e2b-4b38-a6bf-2b62565af24e",
   "metadata": {},
   "source": [
    "Now, we calculate the precision and recall of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05c02aab-bdb8-4abe-8bd2-16243b9e61b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Precision: 0.919220055710306\"\n"
     ]
    }
   ],
   "source": [
    "precision <- pulsar_predictions |> \n",
    "    precision(truth = Class, estimate = .pred_class, event_level = \"second\") |> \n",
    "    pull(.estimate)\n",
    "\n",
    "print(paste(\"Precision:\", precision)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba0588-7a59-4ddb-b40e-e2768186cf19",
   "metadata": {},
   "source": [
    "Calculating the recall of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91d499be-b744-4431-9dce-59335fc4dbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Recall 0.829145728643216\"\n"
     ]
    }
   ],
   "source": [
    "recall <- pulsar_predictions |> \n",
    "    recall(truth = Class, estimate = .pred_class, event_level = \"second\") |> \n",
    "    pull(.estimate)\n",
    "print(paste(\"Recall\", recall))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6f46f4-d757-4241-9b9a-92b00819c646",
   "metadata": {},
   "source": [
    "Finally, let's visualize our results using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327455c9-d173-4157-aa5c-d62ae7548cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_versus_k <- ggplot(mnist_acc, aes(x = neighbors, y = mean))+\n",
    "      geom_point() +\n",
    "      geom_line() +\n",
    "      labs(x = \"Neighbors\", y = \"Accuracy Estimate\") +\n",
    "      scale_x_continuous(breaks = seq(0, 14, by = 1)) +  # adjusting the x-axis\n",
    "      scale_y_continuous(limits = c(0.4, 1.0)) # adjusting the y-axis\n",
    "accuracy_versus_k"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "090261b9-d33f-42e1-b6ca-099d62e32c66",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "### Summary\n",
    "From our analysis of the data set, using K nearest neighbour (KNN) algorithm, we found out that K=30 neighbours gives the optimal accuracy for our model. Our model has a high recall of 91%, and since models with high recall usually have low precision, the precision of our model is a modest 67%\n",
    "\n",
    "### Were these results what we expected?\n",
    "Currently, most ML algorithms that are used to detect pulsars have an accuracy of above 90% (Rong Song, 2023). Hence, we were expecting our model to have an accuacy of above 90% as well. Our model, which uses the K-nearest neighbour (KNN) algorithm and uses 5 variables, has an accuracy of 95%, just as we expected\n",
    "\n",
    "### Impacts of our finding\n",
    "This data analysis can help scientists in classifying if an observation is a Pulsar star. By reliably identifying Pulsars, their periodic nature can give astronomers valuable insight into accurate time intervals at which we are in. This also shows the abundance of gravitational radiation and is a matter of great scientific interest due to the nature of the stars as gravitational probes. act as incredibly accurate timing devices and, as such, have proven to be valuable tools of astronomical discovery\n",
    "\n",
    "### Future questions this data analysis could lead to:\n",
    "\n",
    "- Are there any other factors that can help in determining whether an observation is a Pulsar star?\n",
    "- Are there other algorithms that may be more effective in classifying Pulsar status compared to K-NN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d0d313-121a-4f9c-a113-1cdea9d6778c",
   "metadata": {},
   "source": [
    "### <center> References </center>\r\n",
    "Eatough, R. P., Molkenthin, N., Kramer, M., Noutsos, A., Keith, M. J., Stappers, B. W., & Lyne, A. G. (2010). Selection of radio pulsar candidates using artificial neural networks. Monthly Notices of the Royal Astronomical Society, 407(4), 2443–2450. https://doi.org/10.1111/j.1365-2966.2010.17082.x\n",
    "\n",
    "\n",
    "Ng, C., Champion, D. J., Bailes, M., Barr, E. D., Bates, S. D., Bhat, N. D. R., Burgay, M., Burke-Spolaor, S., Flynn, C. M. L., Jameson, A., Johnston, S., Keith, M. J., Kramer, M., Levin, L., Petroff, E., Possenti, A., Stappers, B. W., van Straten, W., Tiburzi, C., & Eatough, R. P. (2015). The High Time Resolution Universe Pulsar Survey – XII. Galactic plane acceleration search and the discovery of 60 pulsars. Monthly Notices of the Royal Astronomical Society, 450(3), 2922–2947. https://doi.org/10.1093/mnras/stv75 <br> <br>3\n",
    "Galli, S. (2023, November 3). Exploring Oversampling Techniques for Imbalanced Datasets. Train in Data Blog. https://www.blog.trainindata.com/oversampling-techniques-for-imbalanced-data/#:~:text=Oversampling%20is%20a%20data%20augmentation,samples%20in%20the%20minority%20clas \n",
    "<br><br>\n",
    "Song, J. R. (2023). The effectiveness of different machine learning algorithms in classifying pulsar stars and the impact of data preparation. Journal of Physics: Conference Series, 2428(1), 012046. https://doi.org/10.1088/1742-6596/2428/1/012046\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a1912-918a-49a4-b803-6f1821d073dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
